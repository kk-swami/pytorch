{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb134bc-628e-49c6-b7d6-04abc9bfbf1d",
   "metadata": {},
   "source": [
    "Data set and data loader classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de8771-07b5-41fc-85a3-f160b6a28c0d",
   "metadata": {},
   "source": [
    "Unlike the earlier Lin  and Logistic regression modules where training happenend on all the data at once per epoch, typically that's too much data at one shot, we rarely compute loss across all the data in reality. We divide data set into batches and compute per batch and do grad descent per batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de25502b-804d-4ab0-a821-acfeae19f019",
   "metadata": {},
   "source": [
    "for epoch in range(1000):\n",
    "    for i in range(total_batches):\n",
    "        x_batch, y_batch = ...\n",
    "        \n",
    "code is something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b6f90-5d68-4958-9102-63a3a563bbb1",
   "metadata": {},
   "source": [
    "The pytorch dataset and data loader make batch training easier"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d676fcc0-5fc8-4187-85f6-7bc81d90f206",
   "metadata": {},
   "source": [
    "Definitions\n",
    "\n",
    "1) Epoch : 1 epoch is 1 forward and backward pass across the whole data\n",
    "2) batch_size : number of training samples in 1 batch\n",
    "3) Number of iterations : total number of passes, each pass containing training data size = batch size\n",
    "\n",
    "Example : 100 samples totally, batch size 20 => 5 iterations per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0da0f645-eb46-4b87-8bdb-410621db3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fbae6a9-5bf5-4641-9fbc-15830d4f82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement own custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bff67e-77fe-4dda-ae69-3257696ad469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## data loading\n",
    "        xy = np.loadtxt('wine.txt', delimiter = \",\", dtype = np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1: ]) ## all columns except first column\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) ## n_samples*1\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        ## length of dataset\n",
    "        return self.n_samples\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb87cc0-94a5-4e1e-9a48-a11e9859d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a1c9f3-8417-4f9e-bd2a-4eda22ff3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3018c2d5-72a1-488b-826e-dd7ad912e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6658bd5-393d-46b8-a8f1-a30a5c7f0ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
       "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
       "        1.0650e+03])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74228bb-84f6-46f4-b4d9-25f2b569711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5661ee4f-e429-42a4-9394-7b9dca80da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset = dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1854bd2c-4756-4b7f-a556-ba95ea2d40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1e4f0b7-06e4-46c1-a78b-78182510191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4cd4b748-24d6-47ac-a359-d294dda47673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 13])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11a08609-5f87-49ba-b160-0bcbde4205b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d174494-379d-4cae-867e-7cf44336306a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2520e+01, 2.4300e+00, 2.1700e+00, 2.1000e+01, 8.8000e+01, 2.5500e+00,\n",
      "         2.2700e+00, 2.6000e-01, 1.2200e+00, 2.0000e+00, 9.0000e-01, 2.7800e+00,\n",
      "         3.2500e+02],\n",
      "        [1.1840e+01, 2.8900e+00, 2.2300e+00, 1.8000e+01, 1.1200e+02, 1.7200e+00,\n",
      "         1.3200e+00, 4.3000e-01, 9.5000e-01, 2.6500e+00, 9.6000e-01, 2.5200e+00,\n",
      "         5.0000e+02],\n",
      "        [1.2530e+01, 5.5100e+00, 2.6400e+00, 2.5000e+01, 9.6000e+01, 1.7900e+00,\n",
      "         6.0000e-01, 6.3000e-01, 1.1000e+00, 5.0000e+00, 8.2000e-01, 1.6900e+00,\n",
      "         5.1500e+02],\n",
      "        [1.2330e+01, 9.9000e-01, 1.9500e+00, 1.4800e+01, 1.3600e+02, 1.9000e+00,\n",
      "         1.8500e+00, 3.5000e-01, 2.7600e+00, 3.4000e+00, 1.0600e+00, 2.3100e+00,\n",
      "         7.5000e+02]])\n"
     ]
    }
   ],
   "source": [
    "print(features) ## see 4 samples features as batch size is 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95f66c89-d358-4181-9c46-0a4e263c6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dummy training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a773df77-804d-43b3-b4b4-5d7e6682dd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/batch_size) ## total samples/number of batches per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1647d85d-52f3-4d9c-80be-69c3c4b45e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n"
     ]
    }
   ],
   "source": [
    "print(total_samples, n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7021334-f1ff-4929-8314-93f751495d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch : 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch : 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch : 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        \n",
    "        ## forward , backward pass\n",
    "        if (i+1)%5 == 0:\n",
    "            print(f'epoch : {epoch + 1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1500df-98be-484b-a3ca-d1333182e098",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e08a5f3-6e33-451f-a49e-207f325c0970",
   "metadata": {},
   "source": [
    "The ability to apply transformations on dataset\n",
    "\n",
    "Very relevant for images (cropping, rotation etc) but also for tensors in general\n",
    "\n",
    "You can also compose multiple transforms\n",
    "\n",
    "Pytorch natively supports a lot of transforms https://pytorch.org/tutorials/beginner/basics/transforms_tutorial.html\n",
    "\n",
    "In addition, we can write custom transforms like below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6416fdc-0123-4d7d-9b11-321215352c3b",
   "metadata": {},
   "source": [
    "Let's take the same wine data set class as before, but comment out the torch conversion steps, we will do that in transform instead. Add a transform  argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "984294d0-376f-4d70-bfba-ca9f209e7515",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, transform=None):\n",
    "        ## data loading\n",
    "        xy = np.loadtxt('wine.txt', delimiter = \",\", dtype = np.float32, skiprows=1)\n",
    "        ##self.x = torch.from_numpy(xy[:, 1: ]) ## all columns except first column\n",
    "        ##self.y = torch.from_numpy(xy[:, [0]]) ## n_samples*1\n",
    "        self.x =(xy[:, 1:])\n",
    "        self.y =(xy[:, [0]])\n",
    "        self.n_samples = xy.shape[0]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index): ## transform and return if transform available\n",
    "        sample = self.x[index], self.y[index]\n",
    "        sample = self.transform(sample)\n",
    "        return sample\n",
    "        \n",
    "    def __len__(self):\n",
    "        ## length of dataset\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5422009c-5a55-4fcf-9f35-cf34f3610b08",
   "metadata": {},
   "source": [
    "## define a custom transform class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "779a775c-80e7-45ea-b302-c7f1b3571b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor:\n",
    "    \n",
    "    def __call__(self, sample): ## the call method is a special method like init, which allows the object to directly be used to call a function rather than a method\n",
    "        inputs , targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4949a6e-a48d-44a1-ac01-f6e04cf575e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = WineDataset(transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a5ecde6-b95c-40fb-a31b-f24449dbe528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "39e5a37d-efc5-4885-b823-90235bdc8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3c90508-7072-4658-a4a3-f073d8164103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0ef9916-4d94-4cdb-bb60-d57abfbb3ee7",
   "metadata": {},
   "source": [
    "## using compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5adbba4a-8351-445d-b7cf-0056d2cf551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulTransform:\n",
    "    \n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs = inputs*self.factor\n",
    "        return inputs, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e9a5023b-6e71-4c90-add2-a8bac5efedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor(14.2300)\n",
      "<class 'torch.Tensor'>\n",
      "tensor(28.4600)\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataset(transform=ToTensor())\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features))\n",
    "print(features[0])\n",
    "\n",
    "\n",
    "composed = torchvision.transforms.Compose([ToTensor(), MulTransform(2)])\n",
    "dataset = WineDataset(transform = composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(type(features))\n",
    "\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02f21c7c-cdb5-494b-a136-e77a7b10424a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18c8e5-649b-4d68-a3d5-77e74304a0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99df49f9-08f0-4d26-95f8-5d3a9306065e",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5254a-3d7c-4acc-8715-1cb427f67aae",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=PXOzkkB5eH0&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25511be5-b024-44f0-9353-8619fc853f0c",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=X_QOZEko5uE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
