{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db16670-5465-4348-bcaf-fc5af145552c",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "In the context of CV, take a pretrained 1000 class resnet image classifier and finetune it for our two class use case (classify images of bees vs ants using hymenoptera data)\n",
    "\n",
    "In addition, we will also learn about using datasets.ImageFolder, and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9860e08a-52fc-4209-b489-bcd29b62f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03512419-ec15-43c8-89c2-04040e80e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298e0068-7c6f-48ac-a3c1-8be678fbe19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean  = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "## 3 values because normalization across 3 channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbdc69-5d99-4b44-94a1-23ffe0ac8cd8",
   "metadata": {},
   "source": [
    "## Define data transformations : slightly different transforms for train and val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "031fbebf-f043-4431-841b-1fc3b8875d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "  'train' : transforms.Compose([\n",
    "      transforms.RandomResizedCrop(224),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean, std)\n",
    "     \n",
    " ]),\n",
    "  'val' : transforms.Compose([\n",
    "      \n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean, std)\n",
    "      \n",
    "  ])\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25adc79d-c55d-429c-83d3-d05fc033fe1e",
   "metadata": {},
   "source": [
    "## Import data. use datasets.ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55767002-db9b-4eb9-97c9-c130249c2cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'data/hymenoptera_data'\n",
    "sets = ['train', 'val']\n",
    "image_datasets = {x : datasets.ImageFolder(os.path.join(datadir,x), data_transforms[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171a3379-7a74-43a5-8abc-b32f45eec394",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {x : len(image_datasets[x]) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cb1c09a-5a58-4876-8cc1-1a751009312b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 244, 'val': 153}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b05fb62-8bed-45c7-9d33-c0c8b0d55270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = image_datasets['train'].classes ## I'm guessing it picks class names from folder names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f637d1-cdad-44b5-9057-ec8d8b6b8014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ants', 'bees']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c117eb-5f37-4fdc-a729-fa62cf21b805",
   "metadata": {},
   "source": [
    "## data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "122615af-1d95-4eb0-bee6-1c555670af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {x : torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=0) for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e5d6e-8942-4ebe-b717-feaabf9b9f40",
   "metadata": {},
   "source": [
    "## function for training and validation\n",
    "Unlike earlier,  have a common method for both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a5e7481-f4e0-4a71-b25a-605315f2b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs):\n",
    "    \n",
    "    since = time.time()\n",
    "    best_model_weights = copy.deepcopy(model.state_dict()) ## why deepcopy ?\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-'*10)\n",
    "        \n",
    "        ## each epoch has training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase=='train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()  ## This is necessary as since we have resnet which has dropouts, batch_norm, etc\n",
    "                              ## some of these need to be switched on only during training and model.train() and model.eval() helps with these\n",
    "                    \n",
    "            running_loss = 0.0\n",
    "            running_correct = 0\n",
    "            \n",
    "            ## iterate over batches within epoch\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                ## forward pass - if train, capture gradients\n",
    "                with torch.set_grad_enabled(phase = 'train'): ## fancier one line way of using torch.no_grad() to handle train and eval\n",
    "                    output = model(inputs)\n",
    "                    loss = criterion(output, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(output, 1)\n",
    "                    \n",
    "                    ## backward prop + optimize only in training\n",
    "                    if phase=='train':\n",
    "                        optimizer.zero_grad() ## zero grad can either be done before loss backward and step or after, doesn't really matter\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                    running_loss = running_loss + loss.item()*inputs.shape[0] ## loss is averaged loss. multiplying by inputs.shape[0] gives total loss (sum instead of average)\n",
    "                    \n",
    "                    running_correct = running_correct + torch.sum(labels==preds)\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  ## train, evaluate and then update the scheduler\n",
    "                                ## note that scheduler operates at the epoch level, not at the batches within epoch level\n",
    "                                ## hence outside for loop\n",
    "            epoch_loss = running_loss/dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase=='val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "            \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    \n",
    "    ## load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "            \n",
    "            \n",
    "                \n",
    "                    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe92b98c-2810-4ea1-82a3-836cb5241cf0",
   "metadata": {},
   "source": [
    "## Load pretrained resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca9dd009-6930-46f1-942d-cbf3b2d55138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkiit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kkiit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\kkiit/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:14<00:00, 3.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e106a5-8c7f-4c7d-977a-809d0cb595dc",
   "metadata": {},
   "source": [
    "## Two methods : Method 1 : Finetune all weights of pretrained model for current classification\n",
    "## Method 2 : Finetune just last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd46c32c-2fb0-42c6-9e77-15e76fffb041",
   "metadata": {},
   "source": [
    "## Method 1 : All weight finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1f639d-7331-40e2-89fd-d97779617165",
   "metadata": {},
   "source": [
    "We now want to replace the last linear layer of model which outputs logits with a new\n",
    "linear layer with 2 class output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ad14d4c-f3ef-47c5-9c7e-ae87b3ab4b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.fc.in_features ## first we find out the number of inputs in the last linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "303da653-8906-464f-a385-19e73826471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(model.fc.out_features) ## note that currently, last layer has 1000 outputs/1000 classes in resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fd5e819-f99e-4a3e-a01c-8eae0e059877",
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace last linear layer with custom linear layer with 2 outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b70becef-3d31-4001-a3aa-322b6f07e47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b0112c-184e-487f-8722-be23ca7e1a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc.out_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4295de95-366f-48d0-b272-365d992e34ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0305f-c600-479e-9db5-447b55369caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## define a linear scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1919b26-1ada-4cda-a7b8-0d56cc678bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d096e-2310-4fa3-8fbf-84360334c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## means that every 7 epochs, learning rate new = gamma * learning rate old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7c9f1-1c10-47fd-84fa-9cc19417293d",
   "metadata": {},
   "source": [
    "## Actual running !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd52144-d781-437f-8229-b7100ee46a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06faca-c1b7-44db-bec8-863bdbe38376",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4372ca-dc2f-4bc0-8f2c-dbe9dc5114ef",
   "metadata": {},
   "source": [
    "## Method 2 : Only last layer finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "713cf24b-526d-457a-be28-e2a7f41845cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkiit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kkiit\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1841bcc-8608-4bb8-88a7-84bfb1a58e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first freeze all layers to backprop using requires_grad=False\n",
    "\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "    \n",
    "## now just reset last layer, that makes only last layer with requires_grad=True\n",
    "num_ftrs = model_conv.fc.in_features ## first we find out the number of inputs in the last linear layer\n",
    "model_conv.fc = nn.Linear(num_fts, 2) ## automatically has requires_grad = True for last layer alone\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9) ## only last layer being optimized, slightly different hyperparam than when optimizing full network\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba2164-ef73-4ce2-9b37-2350e2ee5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method 2 : Only last layer finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65225274-3e2e-45bb-914a-a36cd53b10a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3072e14-db86-4a4a-8c27-e9f4bcaa0bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a90a8-5c63-4990-9a45-64a16145e952",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4a199-9786-4bdd-b401-db2dd1c9d80a",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=K0lWSB2QoIQ&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
