{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90755350-6f79-45b8-8bb8-8b41befe48c4",
   "metadata": {},
   "source": [
    "4 steps\n",
    "\n",
    "1) Step 1: Forward pass : manual implementation, gradients computation : manual, loss computation : manual, parameter updates : manual\n",
    "2) Step 2 : Forward pass : manual implementation, gradients computation : auto grad, loss computation : manual, parameter updates : manual\n",
    "3) Step 3 : Forward pass : manual implementation, gradients computation : auto grad, loss computation : pytorch loss, parameter updates : pytorch optimizer\n",
    "4) Step 4 : Forward pass : pytorch model, gradients computation : auto grad, loss computation : pytorch loss, parameter updates : pytorch optimizer\n",
    "\n",
    "Step 1 is purely manual implementation, step 4 is more or less completely automated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ccea43-d9ae-4b90-8811-f03e2111b19f",
   "metadata": {},
   "source": [
    "## Step 1 (complete scratch)\n",
    "Forward pass : manual implementation, gradients computation : manual, loss computation : manual, parameter updates : manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272de253-2641-49f9-a7d6-6d2746646248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 0.000\n",
      "epoch 1: w = 1.200, loss = 30.00000000\n",
      "epoch 2: w = 1.680, loss = 4.79999924\n",
      "epoch 3: w = 1.872, loss = 0.76800019\n",
      "epoch 4: w = 1.949, loss = 0.12288000\n",
      "epoch 5: w = 1.980, loss = 0.01966083\n",
      "epoch 6: w = 1.992, loss = 0.00314570\n",
      "epoch 7: w = 1.997, loss = 0.00050332\n",
      "epoch 8: w = 1.999, loss = 0.00008053\n",
      "epoch 9: w = 1.999, loss = 0.00001288\n",
      "epoch 10: w = 2.000, loss = 0.00000206\n",
      "Prediction after training : f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# f = w*x (linear regression, ignore bias)\n",
    "X = np.array([1,2,3,4], dtype = np.float32) ## numpy\n",
    "Y = np.array([2,4,6,8], dtype = np.float32)\n",
    "\n",
    "w = 0.0 ## initialize weight to 0 at beginning\n",
    "\n",
    "## model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "## model loss : MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "## gradient\n",
    "## MSE  = 1/N * (w*x - y)**2\n",
    "## dL/dw = 1/N * 2 * (w*x - y)*x\n",
    "\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted -y).mean()\n",
    "\n",
    "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "## training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    ## forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    ## loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    ## gradient\n",
    "    dw = gradient(X, Y, y_pred)\n",
    "    \n",
    "    ## weight update (gradient descent)\n",
    "    w = w - learning_rate*dw\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l :.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {forward(5):.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6020b7ba-47bf-44f8-bd56-646cce39822b",
   "metadata": {},
   "source": [
    "## Step 2 \n",
    "Forward pass : manual implementation, gradients computation : auto grad, loss computation : manual, parameter updates : manual\n",
    "\n",
    "Weird !! In code below, if instead of line w -= learning_rate*(w.grad), we use w = w - learning_rate*(w.grad), we get error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0aeb66-d0b9-4d79-8f0c-30dc066022d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 2: w = 0.555, loss = 21.67499924\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 4: w = 0.956, loss = 11.31448650\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 6: w = 1.246, loss = 5.90623236\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 8: w = 1.455, loss = 3.08308983\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 10: w = 1.606, loss = 1.60939169\n",
      "Prediction after training : f(5) = 8.031\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# f = w*x (linear regression, ignore bias)\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32) ## numpy\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "## model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "## model loss : MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "## gradient\n",
    "## MSE  = 1/N * (w*x - y)**2\n",
    "## dL/dw = 1/N * 2 * (w*x - y)*x\n",
    "\n",
    "\n",
    "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "## training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    ## forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    ## loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    ## gradient (backward pass)\n",
    "    l.backward() ## dl/dw\n",
    "    ##print(w.grad)\n",
    "    \n",
    "    \n",
    "    ## weight update (gradient descent) - this is couched under torch.no_grad since this is not a part of forward pass, so shouldn't be tracked in computational graph\n",
    "    with torch.no_grad():\n",
    "        ##print(w.grad, \"before here\")\n",
    "        w -= learning_rate*(w.grad)\n",
    "        ##print(w.grad, \"here\")\n",
    "        \n",
    "    ## zero grad\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l :.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {forward(5):.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800650e-d645-433c-8c7f-02c8c57f9af4",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Forward pass : manual implementation, gradients computation : auto grad, loss computation : pytorch loss, parameter updates : pytorch optimizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a9ad03e-67d0-4a15-98ed-a499ed93ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.00000000\n",
      "epoch 2: w = 0.555, loss = 21.67499924\n",
      "epoch 3: w = 0.772, loss = 15.66018772\n",
      "epoch 4: w = 0.956, loss = 11.31448650\n",
      "epoch 5: w = 1.113, loss = 8.17471695\n",
      "epoch 6: w = 1.246, loss = 5.90623236\n",
      "epoch 7: w = 1.359, loss = 4.26725292\n",
      "epoch 8: w = 1.455, loss = 3.08308983\n",
      "epoch 9: w = 1.537, loss = 2.22753215\n",
      "epoch 10: w = 1.606, loss = 1.60939169\n",
      "Prediction after training : f(5) = 8.031\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x (linear regression, ignore bias)\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32) ## numpy\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "## model prediction\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "## model loss : MSE\n",
    "\n",
    "\n",
    "## gradient\n",
    "## MSE  = 1/N * (w*x - y)**2\n",
    "## dL/dw = 1/N * 2 * (w*x - y)*x\n",
    "\n",
    "\n",
    "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
    "\n",
    "\n",
    "## training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    ## forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    ## loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    ## gradient (backward pass)\n",
    "    l.backward() ## dl/dw\n",
    "    ##print(w.grad)\n",
    "    \n",
    "    \n",
    "    ## weight update (gradient descent) - this is couched under torch.no_grad since this is not a part of forward pass, so shouldn't be tracked in computational graph\n",
    "    # with torch.no_grad():\n",
    "    #     ##print(w.grad, \"before here\")\n",
    "    #     w -= learning_rate*(w.grad)\n",
    "    #     ##print(w.grad, \"here\")\n",
    "    optimizer.step() \n",
    "    \n",
    "    ## zero grad\n",
    "    ##w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l :.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {forward(5):.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c2aade-b145-4132-bc7d-afff8ea7d372",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Forward pass : pytorch based, gradients computation : auto grad, loss computation : pytorch loss, parameter updates : pytorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26d2023a-3aa2-41b5-ba52-f2cde1616b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = -3.503\n",
      "epoch 1: w = -0.489, loss = 50.18429565\n",
      "epoch 2: w = -0.166, loss = 34.96062088\n",
      "epoch 3: w = 0.104, loss = 24.39641190\n",
      "epoch 4: w = 0.328, loss = 17.06530380\n",
      "epoch 5: w = 0.516, loss = 11.97758293\n",
      "epoch 6: w = 0.672, loss = 8.44650650\n",
      "epoch 7: w = 0.803, loss = 5.99555588\n",
      "epoch 8: w = 0.912, loss = 4.29408836\n",
      "epoch 9: w = 1.003, loss = 3.11267614\n",
      "epoch 10: w = 1.079, loss = 2.29212236\n",
      "epoch 11: w = 1.143, loss = 1.72196805\n",
      "epoch 12: w = 1.196, loss = 1.32556283\n",
      "epoch 13: w = 1.241, loss = 1.04972410\n",
      "epoch 14: w = 1.278, loss = 0.85754842\n",
      "epoch 15: w = 1.310, loss = 0.72342956\n",
      "epoch 16: w = 1.336, loss = 0.62959975\n",
      "epoch 17: w = 1.359, loss = 0.56373054\n",
      "epoch 18: w = 1.378, loss = 0.51726621\n",
      "epoch 19: w = 1.394, loss = 0.48427182\n",
      "epoch 20: w = 1.407, loss = 0.46062833\n",
      "epoch 21: w = 1.419, loss = 0.44347751\n",
      "epoch 22: w = 1.429, loss = 0.43083629\n",
      "epoch 23: w = 1.437, loss = 0.42132896\n",
      "epoch 24: w = 1.445, loss = 0.41400003\n",
      "epoch 25: w = 1.451, loss = 0.40818751\n",
      "epoch 26: w = 1.457, loss = 0.40343118\n",
      "epoch 27: w = 1.461, loss = 0.39941239\n",
      "epoch 28: w = 1.466, loss = 0.39590937\n",
      "epoch 29: w = 1.470, loss = 0.39276895\n",
      "epoch 30: w = 1.473, loss = 0.38988355\n",
      "epoch 31: w = 1.476, loss = 0.38718018\n",
      "epoch 32: w = 1.479, loss = 0.38460681\n",
      "epoch 33: w = 1.482, loss = 0.38212800\n",
      "epoch 34: w = 1.484, loss = 0.37971884\n",
      "epoch 35: w = 1.487, loss = 0.37736219\n",
      "epoch 36: w = 1.489, loss = 0.37504619\n",
      "epoch 37: w = 1.491, loss = 0.37276220\n",
      "epoch 38: w = 1.493, loss = 0.37050474\n",
      "epoch 39: w = 1.495, loss = 0.36826932\n",
      "epoch 40: w = 1.496, loss = 0.36605370\n",
      "epoch 41: w = 1.498, loss = 0.36385560\n",
      "epoch 42: w = 1.500, loss = 0.36167333\n",
      "epoch 43: w = 1.502, loss = 0.35950640\n",
      "epoch 44: w = 1.503, loss = 0.35735372\n",
      "epoch 45: w = 1.505, loss = 0.35521472\n",
      "epoch 46: w = 1.506, loss = 0.35308951\n",
      "epoch 47: w = 1.508, loss = 0.35097736\n",
      "epoch 48: w = 1.510, loss = 0.34887829\n",
      "epoch 49: w = 1.511, loss = 0.34679177\n",
      "epoch 50: w = 1.513, loss = 0.34471795\n",
      "epoch 51: w = 1.514, loss = 0.34265676\n",
      "epoch 52: w = 1.516, loss = 0.34060794\n",
      "epoch 53: w = 1.517, loss = 0.33857122\n",
      "epoch 54: w = 1.519, loss = 0.33654711\n",
      "epoch 55: w = 1.520, loss = 0.33453470\n",
      "epoch 56: w = 1.521, loss = 0.33253452\n",
      "epoch 57: w = 1.523, loss = 0.33054635\n",
      "epoch 58: w = 1.524, loss = 0.32856998\n",
      "epoch 59: w = 1.526, loss = 0.32660556\n",
      "epoch 60: w = 1.527, loss = 0.32465288\n",
      "epoch 61: w = 1.529, loss = 0.32271188\n",
      "epoch 62: w = 1.530, loss = 0.32078236\n",
      "epoch 63: w = 1.531, loss = 0.31886446\n",
      "epoch 64: w = 1.533, loss = 0.31695804\n",
      "epoch 65: w = 1.534, loss = 0.31506297\n",
      "epoch 66: w = 1.536, loss = 0.31317922\n",
      "epoch 67: w = 1.537, loss = 0.31130674\n",
      "epoch 68: w = 1.538, loss = 0.30944562\n",
      "epoch 69: w = 1.540, loss = 0.30759552\n",
      "epoch 70: w = 1.541, loss = 0.30575639\n",
      "epoch 71: w = 1.543, loss = 0.30392829\n",
      "epoch 72: w = 1.544, loss = 0.30211118\n",
      "epoch 73: w = 1.545, loss = 0.30030492\n",
      "epoch 74: w = 1.547, loss = 0.29850939\n",
      "epoch 75: w = 1.548, loss = 0.29672477\n",
      "epoch 76: w = 1.549, loss = 0.29495072\n",
      "epoch 77: w = 1.551, loss = 0.29318729\n",
      "epoch 78: w = 1.552, loss = 0.29143429\n",
      "epoch 79: w = 1.553, loss = 0.28969181\n",
      "epoch 80: w = 1.555, loss = 0.28795978\n",
      "epoch 81: w = 1.556, loss = 0.28623816\n",
      "epoch 82: w = 1.557, loss = 0.28452680\n",
      "epoch 83: w = 1.559, loss = 0.28282565\n",
      "epoch 84: w = 1.560, loss = 0.28113481\n",
      "epoch 85: w = 1.561, loss = 0.27945393\n",
      "epoch 86: w = 1.563, loss = 0.27778295\n",
      "epoch 87: w = 1.564, loss = 0.27612221\n",
      "epoch 88: w = 1.565, loss = 0.27447131\n",
      "epoch 89: w = 1.567, loss = 0.27283022\n",
      "epoch 90: w = 1.568, loss = 0.27119920\n",
      "epoch 91: w = 1.569, loss = 0.26957759\n",
      "epoch 92: w = 1.570, loss = 0.26796579\n",
      "epoch 93: w = 1.572, loss = 0.26636371\n",
      "epoch 94: w = 1.573, loss = 0.26477119\n",
      "epoch 95: w = 1.574, loss = 0.26318824\n",
      "epoch 96: w = 1.576, loss = 0.26161468\n",
      "epoch 97: w = 1.577, loss = 0.26005036\n",
      "epoch 98: w = 1.578, loss = 0.25849560\n",
      "epoch 99: w = 1.579, loss = 0.25695023\n",
      "epoch 100: w = 1.581, loss = 0.25541395\n",
      "Prediction after training : f(5) = 9.136\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x (linear regression, ignore bias)\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32) ## x and y have to have  a slightly different shape than before. 2D tensor instead of 1-D where no of rows is number of samples, each row has features\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "input_size = n_features\n",
    "output_size = n_features ## in this case\n",
    "# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# ## model prediction\n",
    "# def forward(x):\n",
    "#     return w*x\n",
    "\n",
    "\n",
    "## \n",
    "model = nn.Linear(input_size, output_size) ## model is simple as it is just a linear model\n",
    "## model loss : MSE\n",
    "\n",
    "\n",
    "## gradient\n",
    "## MSE  = 1/N * (w*x - y)**2\n",
    "## dL/dw = 1/N * 2 * (w*x - y)*x\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "\n",
    "## training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "## optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    ## forward pass\n",
    "    ## y_pred = forward(X)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    ## loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    ## gradient (backward pass)\n",
    "    l.backward() ## dl/dw\n",
    "    ##print(w.grad)\n",
    "    \n",
    "    \n",
    "    ## weight update (gradient descent) - this is couched under torch.no_grad since this is not a part of forward pass, so shouldn't be tracked in computational graph\n",
    "    # with torch.no_grad():\n",
    "    #     ##print(w.grad, \"before here\")\n",
    "    #     w -= learning_rate*(w.grad)\n",
    "    #     ##print(w.grad, \"here\")\n",
    "    optimizer.step() \n",
    "    \n",
    "    ## zero grad\n",
    "    ##w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l :.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40971e9b-320a-4317-bb64-78cdb25793bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "[w,b] = model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01d14685-f0ed-4b75-b6b8-e72497cd10bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6433], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8167884a-0840-40d9-9131-14e083f819d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.3674]], requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84078948-51b5-4ac0-9a02-61d84ed666cb",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "Forward pass : pytorch based, gradients computation : auto grad, loss computation : pytorch loss, parameter updates : pytorch optimizer\n",
    "\n",
    "Only difference from step 4  - instead of using nn.Linear directly for the model,\n",
    "define a model class, which is a more general way of doing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4de30d38-ccc1-4aad-8ac0-089fe57c0719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training : f(5) = -5.758\n",
      "epoch 1: w = -0.507, loss = 79.58591461\n",
      "epoch 2: w = -0.101, loss = 55.22529984\n",
      "epoch 3: w = 0.238, loss = 38.32197189\n",
      "epoch 4: w = 0.521, loss = 26.59309196\n",
      "epoch 5: w = 0.756, loss = 18.45466042\n",
      "epoch 6: w = 0.952, loss = 12.80756187\n",
      "epoch 7: w = 1.115, loss = 8.88914585\n",
      "epoch 8: w = 1.251, loss = 6.17022610\n",
      "epoch 9: w = 1.364, loss = 4.28361130\n",
      "epoch 10: w = 1.459, loss = 2.97451663\n",
      "epoch 11: w = 1.538, loss = 2.06615067\n",
      "epoch 12: w = 1.603, loss = 1.43584061\n",
      "epoch 13: w = 1.658, loss = 0.99846941\n",
      "epoch 14: w = 1.704, loss = 0.69497383\n",
      "epoch 15: w = 1.742, loss = 0.48437160\n",
      "epoch 16: w = 1.773, loss = 0.33822656\n",
      "epoch 17: w = 1.800, loss = 0.23680639\n",
      "epoch 18: w = 1.822, loss = 0.16642073\n",
      "epoch 19: w = 1.840, loss = 0.11756900\n",
      "epoch 20: w = 1.856, loss = 0.08365937\n",
      "epoch 21: w = 1.868, loss = 0.06011757\n",
      "epoch 22: w = 1.879, loss = 0.04377011\n",
      "epoch 23: w = 1.888, loss = 0.03241476\n",
      "epoch 24: w = 1.896, loss = 0.02452325\n",
      "epoch 25: w = 1.902, loss = 0.01903527\n",
      "epoch 26: w = 1.907, loss = 0.01521534\n",
      "epoch 27: w = 1.912, loss = 0.01255271\n",
      "epoch 28: w = 1.915, loss = 0.01069328\n",
      "epoch 29: w = 1.918, loss = 0.00939117\n",
      "epoch 30: w = 1.921, loss = 0.00847589\n",
      "epoch 31: w = 1.923, loss = 0.00782911\n",
      "epoch 32: w = 1.925, loss = 0.00736865\n",
      "epoch 33: w = 1.927, loss = 0.00703755\n",
      "epoch 34: w = 1.928, loss = 0.00679632\n",
      "epoch 35: w = 1.929, loss = 0.00661751\n",
      "epoch 36: w = 1.930, loss = 0.00648205\n",
      "epoch 37: w = 1.931, loss = 0.00637674\n",
      "epoch 38: w = 1.932, loss = 0.00629246\n",
      "epoch 39: w = 1.933, loss = 0.00622280\n",
      "epoch 40: w = 1.933, loss = 0.00616336\n",
      "epoch 41: w = 1.934, loss = 0.00611110\n",
      "epoch 42: w = 1.934, loss = 0.00606388\n",
      "epoch 43: w = 1.935, loss = 0.00602018\n",
      "epoch 44: w = 1.935, loss = 0.00597903\n",
      "epoch 45: w = 1.935, loss = 0.00593972\n",
      "epoch 46: w = 1.936, loss = 0.00590173\n",
      "epoch 47: w = 1.936, loss = 0.00586472\n",
      "epoch 48: w = 1.936, loss = 0.00582844\n",
      "epoch 49: w = 1.936, loss = 0.00579279\n",
      "epoch 50: w = 1.937, loss = 0.00575758\n",
      "epoch 51: w = 1.937, loss = 0.00572275\n",
      "epoch 52: w = 1.937, loss = 0.00568826\n",
      "epoch 53: w = 1.937, loss = 0.00565406\n",
      "epoch 54: w = 1.938, loss = 0.00562011\n",
      "epoch 55: w = 1.938, loss = 0.00558642\n",
      "epoch 56: w = 1.938, loss = 0.00555296\n",
      "epoch 57: w = 1.938, loss = 0.00551971\n",
      "epoch 58: w = 1.938, loss = 0.00548668\n",
      "epoch 59: w = 1.939, loss = 0.00545385\n",
      "epoch 60: w = 1.939, loss = 0.00542123\n",
      "epoch 61: w = 1.939, loss = 0.00538880\n",
      "epoch 62: w = 1.939, loss = 0.00535659\n",
      "epoch 63: w = 1.939, loss = 0.00532456\n",
      "epoch 64: w = 1.940, loss = 0.00529271\n",
      "epoch 65: w = 1.940, loss = 0.00526107\n",
      "epoch 66: w = 1.940, loss = 0.00522960\n",
      "epoch 67: w = 1.940, loss = 0.00519835\n",
      "epoch 68: w = 1.940, loss = 0.00516726\n",
      "epoch 69: w = 1.941, loss = 0.00513637\n",
      "epoch 70: w = 1.941, loss = 0.00510566\n",
      "epoch 71: w = 1.941, loss = 0.00507513\n",
      "epoch 72: w = 1.941, loss = 0.00504479\n",
      "epoch 73: w = 1.941, loss = 0.00501462\n",
      "epoch 74: w = 1.941, loss = 0.00498465\n",
      "epoch 75: w = 1.942, loss = 0.00495484\n",
      "epoch 76: w = 1.942, loss = 0.00492523\n",
      "epoch 77: w = 1.942, loss = 0.00489577\n",
      "epoch 78: w = 1.942, loss = 0.00486650\n",
      "epoch 79: w = 1.942, loss = 0.00483740\n",
      "epoch 80: w = 1.942, loss = 0.00480847\n",
      "epoch 81: w = 1.943, loss = 0.00477973\n",
      "epoch 82: w = 1.943, loss = 0.00475116\n",
      "epoch 83: w = 1.943, loss = 0.00472275\n",
      "epoch 84: w = 1.943, loss = 0.00469451\n",
      "epoch 85: w = 1.943, loss = 0.00466644\n",
      "epoch 86: w = 1.943, loss = 0.00463854\n",
      "epoch 87: w = 1.944, loss = 0.00461082\n",
      "epoch 88: w = 1.944, loss = 0.00458324\n",
      "epoch 89: w = 1.944, loss = 0.00455584\n",
      "epoch 90: w = 1.944, loss = 0.00452860\n",
      "epoch 91: w = 1.944, loss = 0.00450153\n",
      "epoch 92: w = 1.944, loss = 0.00447460\n",
      "epoch 93: w = 1.945, loss = 0.00444787\n",
      "epoch 94: w = 1.945, loss = 0.00442126\n",
      "epoch 95: w = 1.945, loss = 0.00439484\n",
      "epoch 96: w = 1.945, loss = 0.00436855\n",
      "epoch 97: w = 1.945, loss = 0.00434245\n",
      "epoch 98: w = 1.945, loss = 0.00431647\n",
      "epoch 99: w = 1.946, loss = 0.00429067\n",
      "epoch 100: w = 1.946, loss = 0.00426502\n",
      "Prediction after training : f(5) = 9.888\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# f = w*x (linear regression, ignore bias)\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype = torch.float32) ## x and y have to have  a slightly different shape than before. 2D tensor instead of 1-D where no of rows is number of samples, each row has features\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype = torch.float32)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "input_size = n_features\n",
    "output_size = n_features ## in this case\n",
    "# w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# ## model prediction\n",
    "# def forward(x):\n",
    "#     return w*x\n",
    "\n",
    "\n",
    "## \n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "        \n",
    "##model = nn.Linear(input_size, output_size) ## model is simple as it is just a linear model\n",
    "model = LinearRegression(input_size, output_size)\n",
    "## model loss : MSE\n",
    "\n",
    "\n",
    "## gradient\n",
    "## MSE  = 1/N * (w*x - y)**2\n",
    "## dL/dw = 1/N * 2 * (w*x - y)*x\n",
    "\n",
    "X_test = torch.tensor([5], dtype = torch.float32)\n",
    "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "\n",
    "## training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "## optimizer = torch.optim.SGD([w], lr = learning_rate)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    ## forward pass\n",
    "    ## y_pred = forward(X)\n",
    "    y_pred = model(X)\n",
    "    \n",
    "    ## loss\n",
    "    l = loss(Y, y_pred)\n",
    "    \n",
    "    ## gradient (backward pass)\n",
    "    l.backward() ## dl/dw\n",
    "    ##print(w.grad)\n",
    "    \n",
    "    \n",
    "    ## weight update (gradient descent) - this is couched under torch.no_grad since this is not a part of forward pass, so shouldn't be tracked in computational graph\n",
    "    # with torch.no_grad():\n",
    "    #     ##print(w.grad, \"before here\")\n",
    "    #     w -= learning_rate*(w.grad)\n",
    "    #     ##print(w.grad, \"here\")\n",
    "    optimizer.step() \n",
    "    \n",
    "    ## zero grad\n",
    "    ##w.grad.zero_()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch%1==0:\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1}: w = {w[0][0].item():.3f}, loss = {l :.8f}')\n",
    "        \n",
    "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d2fbae-5c5c-43d0-bc06-72b6dc556203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7fd2da81-5cc0-4e39-96ca-3c0c41173c10",
   "metadata": {},
   "source": [
    "1) https://www.youtube.com/watch?v=E-I2DNVzQLg&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
