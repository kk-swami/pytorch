{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55113ff9-cf60-4bc6-9a4b-aee6173dec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6c0658-f2f3-4c94-9d8c-56d346a79bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## to compute gradient of function wrt x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad84ee7d-4911-40af-abd8-4674e85ca229",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b14e8e0-271f-4bc9-82cb-f36a15bf0da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3706, -0.1904,  0.0520], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce0ea139-aae1-41b8-a69e-fc48ebacd48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2 ## creates computational graph"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9989351f-5ae4-4246-9c84-82815295ae3b",
   "metadata": {},
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027302a1-9788-4ef7-8f75-a0deac323fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.8474, 2.5126, 1.6623], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(y) ## has attribute grad_fn, a gradient function which allows us to compute gradients during backprop. Here grad function is addbackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b671d49-e2a3-4650-ae45-167e6f792eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y*y*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11ea60a7-41ed-407a-b028-85883764deef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([28.0146,  2.5764, 24.0387], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z) ## here grad function is mulbackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a47181c6-ccd4-48b7-9b0d-7059ab2537d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = z.mean() ## mean function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e624754-83e4-4b35-ac4d-ecdec6479ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4560, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(z) ## here grad function is meanbackward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e48710b3-5d58-42f5-a28b-d7f69671ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward() ## computes gradient of z wrt x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff31c4dc-16e9-4382-8b71-59413bc5224e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.9902, 1.5133, 4.6225])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad) ## Non-none value as x is leaf node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "805e3d4a-c473-44d5-8106-45f818014026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkiit\\AppData\\Local\\Temp\\ipykernel_24276\\1324337507.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(y.grad)\n"
     ]
    }
   ],
   "source": [
    "print(y.grad) ## None as y is not a leaf node"
   ]
  },
  {
   "cell_type": "raw",
   "id": "159407e0-1378-4973-bfa0-e55f8d3fafb5",
   "metadata": {},
   "source": [
    "Note that .grad exists only for leaf nodes ie nodes which are not obtained by another gradient operation\n",
    "If you want to keep gradients at intermediate steps, there is a retain_grad() function to help with that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54a83e83-71f5-46a8-a216-3c95bb790ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6bd38ff2-5955-44e0-8867-e98777d536e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.7426, 1.1350, 3.4669], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "add17f6c-be5f-43b3-b5b8-a051782f35a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y*y*2\n",
    "z = z.mean()\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3f52cfc-f09f-49b1-a1e5-05db97164c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9902, 1.5133, 4.6225])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad ## Now you can see that grad is stored for y (a non-leaf tensor) because of the retain grad operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb942090-218c-4b9f-beed-0e890899bf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## .backward syntax"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e0046f72-1876-4cf6-ab11-6dd5aa54cf92",
   "metadata": {},
   "source": [
    "If the final value with which all gradients are computed is a scalar, like z above or\n",
    "in a typical loss, then just z.backward() is enough to calculate gradients\n",
    "\n",
    "However, for whatever reason, If your final value with which all gradients are computed is a vector\n",
    "you have to do z.backward(v) where v is a vector of some dimension. This is linked to the jacobian, not completely clear currently. Check this https://discuss.pytorch.org/t/clarification-using-backward-on-non-scalars/1059\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca69912c-f8f1-4129-94ad-fd60de21f49c",
   "metadata": {},
   "source": [
    "## clearing gradients after every step\n",
    "It is important to reset gradients after every step to prevent incorrect gradient accumulation\n",
    "If you use pytorch optimizers, this is done automatically using the optimizer.zero_grad() after every step\n",
    "\n",
    "For example, something like\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "optimizer = torch.optim.SGD(weights, lr = 0.01)\n",
    "optimizer.step() ## one optimization step\n",
    "optimizer.zero_grad() ## resets gradients after every step, critical\n",
    "\n",
    "If we don't use optimizer, the following example demonstrates what happens without resetting and what should be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd7404b5-fc4e-4813-aef8-98b735afd32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n",
      "epoch 1\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([6., 6., 6., 6.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(2):\n",
    "    print(\"epoch\", epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    print(model_output)\n",
    "    model_output.backward()\n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a10e9a3d-8999-46de-a8be-f80c20ae980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note that the gradient of w keep (incorrectly) increasing above. It should be only 3, but it becomes 6,\n",
    "## etc as epochs increase, as previous gradient state is not cleared\n",
    "\n",
    "## To clear , do as below (tensor.grad.zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "413e872f-f90c-445f-9361-64ebb3a7153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n",
      "epoch 1\n",
      "tensor(12., grad_fn=<SumBackward0>)\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epoch in range(2):\n",
    "    print(\"epoch\", epoch)\n",
    "    model_output = (weights*3).sum()\n",
    "    print(model_output)\n",
    "    model_output.backward()\n",
    "    print(weights.grad)\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de66ad53-54b3-455f-b24e-7e46906526e0",
   "metadata": {},
   "source": [
    "## Preventing gradient history (even though weight has requires_grad=True, for a sepcific operation, we want \n",
    "## to turn off gradient tracking\n",
    "\n",
    "3 options\n",
    "\n",
    "1) x._requires_grad_(False)\n",
    "2) x.detach()\n",
    "3) with torch.no_grad():\n",
    "        ## write code here\n",
    "        \n",
    "The 3rd operation is most commonly used in inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53b0f029-c0f7-4021-b0ea-83988a1362fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4410, -0.8621, -1.3491], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7c4107be-dfa2-4ab7-9cfb-f37e5f17aa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4410, -0.8621, -1.3491])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da2719-ff96-4e62-9726-3f307f98edfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66daa971-73f1-4637-9534-e16123162e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4410, -0.8621, -1.3491])\n"
     ]
    }
   ],
   "source": [
    "print(x) ## requires grad has been turned off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "665dafd7-c40f-4651-8cd6-eb59e33fe899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4410, -0.8621, -1.3491], requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)  ## turned on again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04d843a5-8cae-4b26-90e9-4688cb4d7bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4410, -0.8621, -1.3491])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.detach_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0680f75-747f-43c3-9435-771ff5d82533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4410, -0.8621, -1.3491])\n"
     ]
    }
   ],
   "source": [
    "print(x) ## turned off requires grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a50a3c08-e696-43bd-8c9c-3d172bd16290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.4410, -0.8621, -1.3491], requires_grad=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c69d916b-feac-40bb-a26c-79814d79faae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "434bb0d3-e391-4efc-b40b-2938110a1edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([ 1.4410, -0.8621, -1.3491], requires_grad=True)\n",
      "y tensor([3.4410, 1.1379, 0.6509])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(\"x\", x)\n",
    "    print(\"y\",y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca413ec4-adbe-44d5-9cbe-95fe699dfd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## y does not have requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd021b-0eef-4c9a-84f3-14a206a3a4aa",
   "metadata": {},
   "source": [
    "## Coding backpropagation\n",
    "simple LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d3bf16f4-165d-4514-84ac-20fb7ac4c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ba16005f-b956-4b9b-ab4a-4ec5b84b91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(1.0, requires_grad=True)  ## initial weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b5fd5e33-e276-4dfe-b95e-927aac624de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## forward_pass\n",
    "\n",
    "y_hat = w*x ## ignoring bias\n",
    "\n",
    "loss = (y_hat - y)**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4896b582-e7c8-4ac3-a3c7-44eca980c846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d024a3eb-a876-48cd-bf88-40fa59d8568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## backward_pass (pytorch computes local gradient and backprop already)\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "45d15e75-99e8-470e-8fb1-6b7648bf97ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "print(w.grad)  ## -2 because dl/dw = dl/dy_hat * dy_hat/dw = 2*(y_hat-y) * x = 2*(-1)*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c9bae917-9d46-4047-b8cd-be682c782976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkiit\\AppData\\Local\\Temp\\ipykernel_24276\\635946740.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:491.)\n",
      "  print(y_hat.grad)\n"
     ]
    }
   ],
   "source": [
    "## update weights\n",
    "## next forward and back ward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58315b86-a425-4a1f-a917-ac8b8c4af66b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098a2aa-c881-4776-b24f-da28ab18eff0",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=DbeIqrwb_dE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=3\n",
    "\n",
    "https://www.youtube.com/watch?v=MswxJw-8PvE\n",
    "\n",
    "https://www.youtube.com/watch?v=3Kb0QS6z7WA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
